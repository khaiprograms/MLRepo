{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaiprograms/MLRepo/blob/main/Week05_Lab_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week 05 Lab: Decision Trees"
      ],
      "metadata": {
        "id": "jB6PZeLtNMqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the previous labs, we will attempt to build our own decision tree."
      ],
      "metadata": {
        "id": "wtiBmJqONTXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we will be using will be that of the iris dataset. The target is to be able to predict the types on flowers in the database."
      ],
      "metadata": {
        "id": "u_Y70mNfV9Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "data = datasets.load_iris()\n",
        "\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8 , random_state=55)"
      ],
      "metadata": {
        "id": "k-z71pFpV9XN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the libraries here."
      ],
      "metadata": {
        "id": "VCzrs-AuT1fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "oRAW6XBm4r2z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two main classes. The first class is the Node class and the second class is the Decision Tree class."
      ],
      "metadata": {
        "id": "rvPky7sSUPZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Node Class"
      ],
      "metadata": {
        "id": "4kF0ceIyApVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a node class that has two functions. The first function is the init function and the second function is the is_leaf function.\n",
        "\n",
        "The init function takes the following parameters:\n",
        "*   feature (to specify the feature)\n",
        "*   threshold (to specify the threshold of the feature)\n",
        "*   left (to keep the left subtree)\n",
        "*   right (to keep the right subtree)\n",
        "*   value (to keep the value of the node. It is None if it is a leaf.)\n",
        "\n",
        "All the parameters should not be essential and be initialized to None in the function prototype. The is_leaf function returns true if the \"value\" parameter in the node is not None."
      ],
      "metadata": {
        "id": "KyOSvGHj6YUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Node:\n",
        "  def __init__(self, TODO):\n",
        "    self.feature = TODO\n",
        "    self.threshold = TODO\n",
        "    self.left = TODO\n",
        "    self.right = TODO\n",
        "    self.value = TODO\n",
        "\n",
        "  def is_leaf_node(self):\n",
        "    return TODO\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "57nERybrIKuI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A4s7Ogan2-uJ"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is all there is to the Node class! Now we start to build the decision tree class."
      ],
      "metadata": {
        "id": "2s4o4h5cA2Tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Decision Tree Class\n",
        "\n",
        "The decision tree class requires the use of some helper functions. The tree requires a few functions and we will try to explain the functions one at a time:\n",
        "*   init\n",
        "*   fit\n",
        "*   build_tree\n",
        "*   best_split\n",
        "*   information_gain\n",
        "*   split\n",
        "*   entropy\n",
        "*   most_common_label\n",
        "*   predict\n",
        "*   traverse_tree\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bzO8myk38JqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.1 Init Function"
      ],
      "metadata": {
        "id": "guNj-A7JBPR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please write the init function below. It takes in 3 parameters, min_samples_split, max_depth and n_features.\n",
        "\n",
        "* min_samples_split is the parameter to decide the minimum number of samples a class has to contain before you do the split.\n",
        "* max_depth is the maximum depth of the tree.\n",
        "* n_features is the number of features the tree is going to receive.\n",
        "\n",
        "Within the function, you should have a root node that is initialized to None."
      ],
      "metadata": {
        "id": "tDjYe6EX5h9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, min_samples_split, max_depth, n_features):\n",
        "  self.min_samples_split= min_samples_split\n",
        "  self.max_depth= max_depth\n",
        "  self.n_features= n_features\n",
        "  self.root = None"
      ],
      "metadata": {
        "id": "6RIdq6ic6aUc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few high level functions that the users will evoke in the decision tree class. The functions are just:\n",
        "\n",
        "* fit\n",
        "* predict"
      ],
      "metadata": {
        "id": "m73Z26fb7nmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.2 Fit Function\n",
        "\n",
        "The fit function takes in X and y variables. X is the observations, with the shape at index 0 is the number of observations and the shape at index 1 is the number of features. The fit function then calls a function known as build_tree which takes X and y as parameters. The build_tree function returns the root of the tree."
      ],
      "metadata": {
        "id": "t5_1eME9BYYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self, X, y):\n",
        "  self.n_features = X.shape[1] if X.ndim > 1 else 1  # Handle if X is a 1D array\n",
        "  self.root = self.build_tree(X, y)"
      ],
      "metadata": {
        "id": "pZZVwA8w9C9f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.3 Build Tree Function"
      ],
      "metadata": {
        "id": "HreUBJTVBbnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will focus on the build_tree function subsequently. After we have completed the build_tree function, the predict function becomes clearer.\n",
        "\n",
        "The build tree is a recursive function takes the following structure.\n",
        "\n",
        "```\n",
        "def build_tree(self, TODO):\n",
        "  # book keeping intialization.\n",
        "\n",
        "  # check stopping criteria. if stopping criteria has been reached we return the root.\n",
        "\n",
        "  # find the best split based on the information gain.\n",
        "\n",
        "  # split the tree to the left and right subtrees recursively.\n",
        "```\n",
        "\n",
        "It takes in three parameters: X, y and depth. Depth takes the default value of zero. This should help you define what the function prototype is.\n",
        "\n",
        "```\n",
        "def build_tree(self, TODO):\n",
        "```"
      ],
      "metadata": {
        "id": "ErA5gS0j9YSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 5.2.3.1 Build Tree Function: Book Keeping\n",
        "\n",
        "The first things we need to do is some book keeping, which is to store some of their basic parameters in the function. We need only three parameters, which is the size number of observations in X, the number of features in X and the number of unique labels in y.\n",
        "\n",
        "```\n",
        "def build_tree(self, TODO):\n",
        "  n, f = TODO\n",
        "  n_labels = TODO\n",
        "\n",
        "```\n",
        "\n",
        "#### 5.2.3.2 Build Tree Function: Stopping Criteria\n",
        "\n",
        "There are three stopping criteria, three reasons why the recursive function should stop. The criteria are\n",
        "* if the depth of the tree is equal or greater than the max depth allowed for the tree.\n",
        "* if there is only 1 feature left in X.\n",
        "* if the number of observations is X  is less than the mininum samples.\n",
        "\n",
        "```\n",
        "def build_tree(self, TODO):\n",
        "  n, f = TODO\n",
        "  n_labels = TODO\n",
        "\n",
        "  # check the stopping criteria\n",
        "  if (TODO or TODO or TODO):\n",
        "```\n",
        "\n",
        "If the stopping criteria is reached, the current X and y values should be used to build the leaf node. The value of the leaf should be the most common class in our y values. The rest of the parameters in the node can be left as just None.\n",
        "\n",
        "To find the most common label, we will call a function known as most_common_label and pass in just the y values. It has been provided for you, however please be familiar with the Counter function:\n",
        "\n",
        "https://realpython.com/python-counter/\n",
        "\n",
        "```\n",
        "def most_common_label(self,y):\n",
        "  counter = Counter(y)\n",
        "  value = counter.most_common(1)[0][0]\n",
        "  return value\n",
        "\n",
        "def build_tree(self, TODO):\n",
        "  n, f = TODO\n",
        "  n_labels = TODO\n",
        "\n",
        "  # check the stopping criteria\n",
        "  if (TODO or TODO or TODO):\n",
        "    leaf_value = TODO\n",
        "    return Node(TODO)\n",
        "```\n",
        "\n",
        "After we have decided that the stopping critia has not been reached, we can find the next feature to split the next subtree on.\n",
        "\n",
        "We first pick the possible features we want to consider the split. If the number of features are very big, sometimes we will use only a subset of these features. However for simplicity, we will just pick the list of features to be considered to be just the number of features in X.\n",
        "    \n",
        "```\n",
        "def build_tree(self, TODO):\n",
        "  n, f = TODO\n",
        "  n_labels = TODO\n",
        "\n",
        "  # check the stopping criteria\n",
        "  if (TODO or TODO or TODO):\n",
        "    leaf_value = TODO\n",
        "    return Node(TODO)\n",
        "\n",
        "  feat_idx = TODO\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zlD72-l9AN2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.3.3 Build Tree Function: Best Split (To be continued)\n",
        "\n",
        "The next portion is the hardest part of the entire build tree function. That is to find the best split of the subtree based on the X, y and feature. This is known as the best_split function. This best_split takes in the variables X, y and the feat_idx. This function returns the best_feature to do the split, and the best_thresh for the split.\n",
        "\n",
        "```\n",
        "def build_tree(self, TODO):\n",
        "  n, f = TODO\n",
        "  n_labels = TODO\n",
        "\n",
        "  # check the stopping criteria\n",
        "  if (TODO or TODO or TODO):\n",
        "    leaf_value = TODO\n",
        "    return Node(TODO)\n",
        "\n",
        "  feat_idx = TODO\n",
        "\n",
        "  # find the best split\n",
        "  TODO, TODO = self.best_split(TODO)\n",
        "\n",
        "  # create the child nodes\n",
        "  left_idxs, right_idxs = self.split(X[:, best_feature], best_thresh)\n",
        "  left = self.build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "  right = self.build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "  return Node(best_feature, best_thresh, left, right)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xFd80uTfM8nU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.3.4 Build Tree Function: Recursive Call\n",
        "\n",
        "Finally we build the child nodes by splitting the current X and y values based on the splitting feature and splitting threshold. These will be used to build the left and the right subtree, and we finally carry out the subsequent recursive call.\n",
        "\n",
        "```\n",
        "def build_tree(self, TODO):\n",
        "  n, f = TODO\n",
        "  n_labels = TODO\n",
        "\n",
        "  # check the stopping criteria\n",
        "  if (TODO or TODO or TODO):\n",
        "    leaf_value = TODO\n",
        "    return Node(TODO)\n",
        "\n",
        "  feat_idx = TODO\n",
        "\n",
        "  # find the best split\n",
        "  TODO, TODO = self.best_split(TODO)\n",
        "\n",
        "  # create the child nodes\n",
        "  left_idxs, right_idxs = self.split(X[:, best_feature], best_thresh)\n",
        "  left = self.build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "  right = self.build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "  return Node(best_feature, best_thresh, left, right)\n",
        "```"
      ],
      "metadata": {
        "id": "BuiWa_9eRAEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide the split function for you. The split function finds the observations whose value in the feature was equal or less than the threshold. These will be placed in the left subtree. The rest of the observations will be placed in the right subtree.\n",
        "\n",
        "```\n",
        "def split(self, X_column, split_thresh):\n",
        "  left_idxs = np.argwhere(X_column<=split_thresh).flatten()\n",
        "  right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "  return left_idxs, right_idxs\n",
        "```"
      ],
      "metadata": {
        "id": "RPFvybEHRRyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.3.5 Build Tree Function: Best Split (Continue here)\n",
        "\n",
        "We continue our discussion of the best split function here. The discussion of the best split function in fact requires the discussion of three different functions:\n",
        "\n",
        "* best_split\n",
        "* information_gain\n",
        "* entropy\n",
        "\n",
        "We start from top down as it is more intuitive to do so. As mentioned, the best_split finds the feature that is best to split the dataset (into the left and right subtrees) by, and also the threshold. Hence we first start by building the skeleton:\n",
        "\n",
        "```\n",
        "def best_split(self, X, y, feat_idxs):\n",
        "  best_gain = -1\n",
        "  split_idx, split_threshold = None, None\n",
        "\n",
        "  TODO: many things happening here.\n",
        "\n",
        "  return split_idx, split_threshold\n",
        "```\n",
        "\n",
        "Lets look at the big TODO section and try to break it down. We will run through the feat_idxs and go feature by feature down the X column. For each X column, we will find the unique values in the X column. These unique values will be the values we will use to split the column by. Hence we will name them as thresholds.\n",
        "\n",
        "```\n",
        "def best_split(self, X, y, feat_idxs):\n",
        "  best_gain = -1\n",
        "  split_idx, split_threshold = None, None\n",
        "\n",
        "  for feat_idx in feat_idxs:\n",
        "    X_column = TODO\n",
        "    thresholds = TODO\n",
        "\n",
        "    TODO: still many things happening here.\n",
        "\n",
        "  return split_idx, split_threshold\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-3iYfcRiTk5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we have the feature we want to analyze within the for loop, as well as the distinct values of that feature. The next is to just do a brute force calculation of the information gain we obtain when we use that value in the threshold. We will just iterate through all the values in the threshold and pick the one with the most information gain.\n",
        "\n",
        "For the time being, please assume that to find the information gain, we use a function information_gain. We will write this function later.\n",
        "\n",
        "\n",
        "```\n",
        "def best_split(self, X, y, feat_idxs):\n",
        "  best_gain = -1\n",
        "  split_idx, split_threshold = None, None\n",
        "\n",
        "  for feat_idx in feat_idxs:\n",
        "    X_column = TODO\n",
        "    thresholds = TODO\n",
        "\n",
        "    #TODO: for loop to iterate through all the tresholds.\n",
        "      gain = self.information_gain(y, X_column, thr)\n",
        "\n",
        "      # if the information gain is greater than best_gain:\n",
        "        # set the best_gain to the current_gain\n",
        "        # set the split_idx to be the feat_idx\n",
        "        # set the split_threshold to be the threshold used to get this best gain.\n",
        "\n",
        "  return split_idx, split_threshold\n",
        "```"
      ],
      "metadata": {
        "id": "A4jsUlbwe6Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we look into this information_gain function! We are almost completing~\n",
        "\n",
        "```\n",
        "  def information_gain(self, y, X_column, threshold):\n",
        "\n",
        "    # first we take split the X_columns using again the split function, to get the indexes on the left and on the right of the tree.\n",
        "\n",
        "    left_idx, right_idx = TODO\n",
        "\n",
        "\n",
        "    # either the length of the left or right indexes is zero, we return 0 for no information gain\n",
        "    if TODO or TODO:\n",
        "      return TODO\n",
        "\n",
        "    # we get the length of y. ie. how many y values they are.\n",
        "    n = TODO\n",
        "\n",
        "    # to find the number of observations on the left / right subtree\n",
        "    n_l, n_r = len(left_idx), len(right_idx)\n",
        "\n",
        "    # use entropy function to calculate the amount of entropy per branch.\n",
        "    e_l, e_r = self.entropy(y[left_idx]), self.entropy(y[right_idx])\n",
        "\n",
        "    # find the amount of entropy in the child. Remember to use the n, n_l, e_l and also n, n_r, e_r.    \n",
        "    child_entropy = TODO\n",
        "\n",
        "    information_gain = 1 - child_entropy\n",
        "    return information_gain\n",
        "```"
      ],
      "metadata": {
        "id": "oIk2TbUGfylA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we come to the last function, to find the entropy. It takes the y values and calculates the entropy of it. There are several ways of calculating the entropy and my technique may not be the best one.\n",
        "\n",
        "```\n",
        "#   def entropy(self, y):\n",
        "#     return entropy_of_y\n",
        "```"
      ],
      "metadata": {
        "id": "Z4vEQUbm8WkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will give you the predict functions, however please understand what the predict functions mean..\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#   def predict(self, X):\n",
        "#     return np.array([self.traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "#   def traverse_tree(self, x, node):\n",
        "#     if node.is_leaf_node():\n",
        "#       return node.value\n",
        "\n",
        "#     if x[node.feature] <= node.threshold:\n",
        "#       return self.traverse_tree(x, node.left)\n",
        "#     return self.traverse_tree(x, node.right)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_Wol-1gyhzqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None"
      ],
      "metadata": {
        "id": "HcADjTANWgCr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "\n",
        "  def __init__(self, min_samples_split=2, max_depth=5, n_features=None):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_features = n_features\n",
        "        self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.n_features = X.shape[1] if X.ndim > 1 else 1  # Handle if X is a 1D array\n",
        "    self.root = self.build_tree(X, y)\n",
        "\n",
        "  def most_common_label(self,y):\n",
        "     counter = Counter(y)\n",
        "     value = counter.most_common(1)[0][0]\n",
        "     return value\n",
        "\n",
        "  def split(self, X_column, split_thresh):\n",
        "     left_idxs = np.argwhere(X_column<=split_thresh).flatten()\n",
        "     right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "     return left_idxs, right_idxs\n",
        "\n",
        "  def build_tree(self, X, y, depth=0):\n",
        "     n, f = X.shape\n",
        "     n_labels = len(np.unique(y))\n",
        "\n",
        "     # check the stopping criteria\n",
        "     if (depth >= self.max_depth or n_labels == 1 or n < self.min_samples_split):\n",
        "       leaf_value = self.most_common_label(y)\n",
        "       return Node(value=leaf_value)\n",
        "\n",
        "     feat_idx = np.random.choice(f, self.n_features, replace=False)\n",
        "\n",
        "     # find the best split\n",
        "     best_feature, best_thresh = self.best_split(X, y, feat_idx)\n",
        "\n",
        "     # create the child nodes\n",
        "     left_idxs, right_idxs = self.split(X[:, best_feature], best_thresh)\n",
        "     left = self.build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "     right = self.build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "     return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "  def best_split(self, X, y, feat_idxs):\n",
        "     best_gain = -1\n",
        "     split_idx, split_threshold = None, None\n",
        "\n",
        "     for feat_idx in feat_idxs:\n",
        "        X_column = X[:, feat_idx]\n",
        "        thresholds = np.unique(X_column)\n",
        "        for threshold in thresholds:\n",
        "          gain = self.information_gain(y, X_column, threshold)\n",
        "          if gain > best_gain:\n",
        "            best_gain = gain\n",
        "            split_idx = feat_idx\n",
        "            split_threshold = threshold\n",
        "     return split_idx, split_threshold\n",
        "\n",
        "  def information_gain(self, y, X_column, threshold):\n",
        "     left_idx, right_idx = self.split(X_column, threshold)\n",
        "     if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "        return 0\n",
        "\n",
        "     n = len(y)\n",
        "     n_l, n_r = len(left_idx), len(right_idx)\n",
        "     e_l, e_r = self.entropy(y[left_idx]), self.entropy(y[right_idx])\n",
        "     child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
        "     information_gain = 1 - child_entropy\n",
        "     return information_gain\n",
        "\n",
        "  def entropy(self, y):\n",
        "     hist = np.bincount(y)\n",
        "     ps = hist / len(y)\n",
        "     return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "     return np.array([self.traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "  def traverse_tree(self, x, node):\n",
        "     if node.is_leaf_node():\n",
        "        return node.value\n",
        "\n",
        "     if x[node.feature] <= node.threshold:\n",
        "       return self.traverse_tree(x, node.left)\n",
        "     else:\n",
        "       return self.traverse_tree(x, node.right)"
      ],
      "metadata": {
        "id": "9rO73r_sXMKs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can run and test the decision tree that we built!"
      ],
      "metadata": {
        "id": "YI1T_SUzWgvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTree()\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "yfKUflwHWLD1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq6KnvatVvyz",
        "outputId": "e5aeda5b-bbfc-4850-fd11-0abfbd6e6e4d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "credits: https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/blob/main/04%20Decision%20Trees/DecisionTree.py"
      ],
      "metadata": {
        "id": "meOy71gsZuEZ"
      }
    }
  ]
}